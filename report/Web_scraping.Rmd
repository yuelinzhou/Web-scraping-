---
title: "Workout3-yuelin-zhou"
output: github_document
---
## Codes are in Rmd file.
# Part 3: Practice with Regular Expressions
```{r echo=FALSE}
library(stringr)
library(tm)
# take out these data from my clean data folder 
# change data type for each column
column_types <- c("character","character","character","real","real")
abhijit <- read.csv("~/stat133/workouts/workout3/data/cleandata/abhijit_banerjee_GoogleScholarCitations.csv",na.strings = "",colClasses = column_types)
angus <- read.csv("~/stat133/workouts/workout3/data/cleandata/angus_deaton_GoogleScholarCitations.csv",na.strings = "",colClasses = column_types)
```
## a) For the two scholars, how many of their paper titles begin with a word that starts with a vowel, respectively?
```{r echo=FALSE}
# creat an vector called vowel contains all vowel words
vowel <- c("a","e","i","o","u")
number1 <- 0
# I use substr to display the first letter of the word and use %in% to check
for (i in 1:nrow(abhijit)){
if (tolower(substr(abhijit$paperName[i],1,1)) %in% vowel == TRUE) {
 number1 <- number1 +1
}
}
number1
```
I creat an vector called vowel contains all vowel words, and I use substr to display the first letter of the word and use %in% to check, then I got the result of 118 paper titles begin with a vowel word in abhihit's data.
```{r echo=FALSE}
# for angus's data, I just repeat the same thing as abhijit
number2 <- 0
for (i in 1:nrow(angus)){
if (tolower(substr(angus$paperName[i],1,1)) %in% vowel == TRUE) {
 number2 <- number2 +1
}
}
number2
```
Do the same thing for angus's data and I got 146 paper titles begin with a vowel word in angus's data

## b) For the two scholars, how many of their paper titles end with “s” respectively?
```{r echo=FALSE}
# for abhijit's data
# I use substr and nchar to display each string in the last character,
# so I can check them in the if statement
number_s1 <- 0
for(i in 1:nrow(abhijit)){
  if(tolower(substr(abhijit$paperName[i],nchar(abhijit$paperName[i]),
                    nchar(abhijit$paperName[i]))) == "s"){
    number_s1 <- number_s1 +1
  }
}
number_s1
```
For abhijit's data, I use substr and nchar to display each string in the last character,so I can check them in the if statement. Finally the result will be 78 paper titles end with “s” in abhijit's data.

```{r echo= FALSE}
# for angus's data, I just repeat the same thing as abhijit
number_s2 <- 0
for(i in 1:nrow(angus)){
  if(tolower(substr(angus$paperName[i],nchar(angus$paperName[i]),
                    nchar(angus$paperName[i]))) == "s"){
    number_s2 <- number_s2 +1
  }
}
number_s2
```
Do the samething for angus's data, I got 127 paper titles end with “s” in angus's data.

## c) For the two scholars, find the longest title, respectively (“longest” in terms of number of characters)
```{r echo= FALSE}
# The longest title for abhijit's data is 178 characters
# I use nchar to make an vector for the titles, and compares them using max()
# also I sue which.max() to find out the index for the longest title
max(nchar(abhijit$paperName))
# their title is 
abhijit$paperName[which.max(nchar(abhijit$paperName))]
```
The longest title for abhijit's data is 178 characters,I use nchar to make an vector for the titles, and compares them using max(), then I use which.max() to find out the index for the longest title, and finally display the title.

```{r echo= FALSE}
# for angus's data, I just repeat the same thing as abhijit
max(nchar(angus$paperName))
# their title is 
angus$paperName[which.max(nchar(angus$paperName))]
```
Same for angus's data, The longest title for angus's data is 177 characters and with the title above.

## d) For the two scholars, calculate the variable “number of punctuation symbols in the their titles”. Display summary() statistics of these variables, and the corresponding histograms
```{r echo = FALSE}
# for abhijit's data
# I make an vector to store each count in each titles
summary_vector_abhijit <- c()
for(i in 1:nrow(abhijit)){
  # regular expression
 summary_vector_abhijit[i] <- str_count(abhijit$paperName[i],"[:punct:]")
}
# display statistics of these variables using summary()
summary(summary_vector_abhijit)

# corresponding histograms
hist(summary_vector_abhijit,
     xlab = "counts",
     main = "Number of punctuation symbols on abhijit ",
     labels = TRUE,
     col = rainbow(21),
     breaks = 21,
     ylim = c(0,350))
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/abhijit_his.png")
```
For abhijit's data, I make an vector to store each count in each titles, using regular expression "[:punct:]" to does the job,then display statistics of these variables using summary(), and finally make an corresponding histograms

```{r echo= FALSE}
# for angus's data
summary_vector_angus <- c()
for(i in 1:nrow(angus)){
  # regular expression
 summary_vector_angus[i] <- str_count(angus$paperName[i],"[:punct:]")
}

# display statistics of these variables using summary()
summary(summary_vector_angus)

# corresponding histograms
hist(summary_vector_angus,
     xlab = "counts",
     main = "Number of punctuation symbols on angus ",
     col = rainbow(15),
     labels = TRUE,
     breaks = 15,
     ylim = c(0,400))
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/angus_his.png")
```
For angus's data, I just repeat the same thing as abhijit and make an corresponding histograms with it

## e) Remove stop words(“the”, “a”, “an”, “and”, “in”, “if”, “but”), numbers and punctua- tions from the titles
### I feel like the stop words(“the”, “a”, “an”, “and”, “in”, “if”, “but”) are not enough, I think the word "of","for","to","from","on" should also be remove.
```{r echo= FALSE}
# I make an vector to store each filtered titles
filtered_title_abhijit <- c()
# a big forloop that can filtered out stopwords,numbers and punctuations
# from each titles
for(i in 1:nrow(abhijit)){
  filtered_title_abhijit[i] <- tolower(abhijit$paperName[i]) %>% 
  # remove stopwords
  removeWords(c("the","a","an", "and", "in", "if", "but","of","for","to","from","on")) %>%
  # remove numbers
  removeNumbers() %>%
  # remove punctuation 
  removePunctuation()
}
# clean up the extra whitespaces using stripWhitespace() and str_trim()
filtered_title_abhijit <- str_trim(stripWhitespace(filtered_title_abhijit))
# display the first 10 line of this data
filtered_title_abhijit[1:10]
```
For abhijit's data, I make everything in lowercase first, then I make an vector to store each filtered titles, then using a big forloop that can filtered out stopwords,numbers and punctuations, then clean up the extra whitespaces using stripWhitespace() and str_trim(), and finally display the first 10 line of this data. 

```{r echo = FALSE}
filtered_title_angus <- c()
for(i in 1:nrow(angus)){
  filtered_title_angus[i] <- tolower(angus$paperName[i]) %>% 
  # remove stopwords
  removeWords(c("the","a","an", "and", "in", "if", "but","of","for","to","from","on")) %>%
  # remove numbers
  removeNumbers() %>%
  # remove punctuation 
  removePunctuation()
}
# clean with the extra whitespaces
filtered_title_angus <-str_trim(stripWhitespace(filtered_title_angus))
# display the first 10 line of this data
filtered_title_angus[1:10]
```
Do the samething with angus's data and finally display the first 10 line of this data. 

## f) Excluding stop words, numbers and punctuations, what are the 10 most frequent words in scholar A’s titles?
```{r echo= FALSE}
# creat an vector that stores all words
words_abhijit <- c()
for(i in 1:nrow(abhijit)){
  # using append to add up the vector
  # use strslipit to display the words from a strings
  words_abhijit <- append(words_abhijit,
    strsplit(filtered_title_abhijit[i]," ")[[1]])
} 
# use sort from highest to lowest, and display from 1 to 10
sort(table(words_abhijit),decreasing = TRUE)[1:10]
```
For abhijit's data, I again creat an vector that stores all words, using append to add up the vector and use strslipit to display the words from a strings, then sort from highest to lowest, and display the top 10 of the data set

## g) Excluding stop words, numbers and punctuations, what are the 10 most frequent words in scholar B’s titles?
```{r echo = FALSE}
# repeat the same thing from previous problem for angus's data
words_angus <- c()
for(i in 1:nrow(angus)){
  words_angus <- append(words_angus,
    strsplit(filtered_title_angus[i]," ")[[1]])
} 
sort(table(words_angus),decreasing = TRUE)[1:10]
```
I did the samething with angus's data and finally display the first 10 line of this data.

# Part 4: Data visualizations
```{r echo=FALSE}
# load these packages
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
```

## 1) Excluding stop words, numbers and punctuations, create two wordclouds for all the titles of scholar A and B respectively. What’s your observation from the wordcloud plots?
```{r echo= FALSE}
# make corpus 
abhijit_corpus <- Corpus(VectorSource(filtered_title_abhijit))
angus_corpus <- Corpus(VectorSource(filtered_title_angus))
# insepct the elements
inspect(abhijit_corpus[1:5])
inspect(angus_corpus[1:5])

# png(filename = "/Users/yuelinzhou/stat133/workouts/workout3/images/abhijit_wordcloud.png",
#     height = 15,width = 20,units = "cm",res = 450)
# wordcloud(abhijit_corpus,
#           scale=c(3,.25),
#           colors = brewer.pal(8,"Dark2"))
# dev.off()
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/abhijit_wordcloud.png")
```
For abhijit's data, I make corpus first then insepct the elements to see if anything wrong,then I start plot my wordcloud,
Based on abhjit's data, I saw that the most frequency words are tend to be bigger,for example the word evidence and india are big. if we set random.order = FALSE (in this case, I didn't), then the more frequently used word will place at the center. as we can see abhijit love to use evidence to support certian point he wants to make, therefore he must took a lot of reference from other scholars.

```{r echo= FALSE}
 # png(filename = "/Users/yuelinzhou/stat133/workouts/workout3/images/angus_wordcloud.png",
 #     height = 15,width = 20,units = "cm",res = 450)
 # wordcloud(angus_corpus,
 #           scale=c(2,.3),
 #           colors = brewer.pal(8,"Accent"))
 # dev.off()
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/angus_wordcloud.png")
```
Do the same thing with angus's data,I make corpus first then insepct the elements to see if anything wrong,then I start plot my wordclouD. For angus's data has more words, so I set the smaller scale, based on angus's data,I Saw that health,household,data and household,health are seem to be the most frequently used words, as we can see that angus is more serious towards the solving problem based on ineuqality on people's health condition, so that we can boost the economic for the society.

## 2) Create a line plot that displays the number of the publications for the two scholars across years. What can you observe from the plot?
```{r echo= FALSE}
# display the unique year for the data set
sort(unique(na.omit(abhijit[,5])))
sort(unique(na.omit(angus[,5])))

# find out how many publication for each year
abhijit_pub <- c()
for (i in 1:length(sort(unique(na.omit(abhijit[,5]))))){
  abhijit_pub[i] <- table(abhijit[,5])[[i]]
}

# do the same thing for angus's data
angus_pub <- c()
for (i in 1:length(sort(unique(na.omit(angus[,5]))))){
  angus_pub[i] <- table(angus[,5])[[i]]
}

# make an dataframe store number of publciation for each year
# also indicated their name
both_data <- data.frame(name = append(rep("abhijit",length(abhijit_pub)),rep("angus",length(angus_pub))),
                        number = append(abhijit_pub,angus_pub),
                        year = append(sort(unique(na.omit(abhijit[,5]))),
                                      sort(unique(na.omit(angus[,5])))))

# make an line plot 
# png(filename = "/Users/yuelinzhou/stat133/workouts/workout3/images/publication_year.png")
# ggplot(both_data,aes(x = year,y = number,group = name,color = name))+
#   geom_line() +
#   xlim(1965,2020) +
#   labs(title ="publication over years on 2 scholars",y = "number of publication")
# dev.off()
```
Before ploting, I first display the unique year for the data set above,then I remove all na value for all unknown year by using na.omit(), then I make an foor loop to find out how many publication for each year, then make an dataframe store number of publciation for each year and also indicated their name in the data frame.
```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/publication_year.png")
```
Based on the data plot above, I discovered both line are increasing as the year increases. Abhijit seems publish more articles than angus in around (2000 to 2019). Angus seems publish more articles than abhijit in the past (1970 to 2000). In addtion, angus's timeline seems more balance that abhijit because he roughly post 14-19 articles each year. Comparing to abhijit, he only post 1 article in the early time, but increase raipdly round 2000 until today, both lines are very interesting.

## 3) For each author, select 3 of the top 10 most frequently used words in his/her titles. With this set of six words, create a plot with timelines that show the trend (i.e. evolution) of each word over a period of 10 to 20 year (the more years the better)

### For abhijit's data I pick poor,economic and evidence
Before ploting, I first display how many times the word appear on each year. For the word poor:
```{r echo= FALSE}
# for the word poor 
poor <- abhijit[str_detect(tolower(abhijit$paperName),"poor"),]
# make an vector stores all years that has "poor"
poor_year <- sort(unique(poor$year))
# make an vector stores how many time this word appear in each year
poor_number <- c()
for(i in 1:length(poor_year)){
 poor_number[i] <- table(poor$year)[[i]]
}
table(poor$year)
```

For the word econmic:
```{r echo =FALSE}
econ <- abhijit[str_detect(tolower(abhijit$paperName),"economic"),]
# make an vector stores all years that has "econmic"
econ_year <- sort(unique(econ$year))
# # make an vector stores how many time this word appear in each year
econ_number <- c()
for(i in 1:length(econ_year)){
 econ_number[i] <- table(econ$year)[[i]]
}
table(econ$year)
```

For the word evidence: 
```{r echo =FALSE}
evi <- abhijit[str_detect(tolower(abhijit$paperName),"evidence"),]
# make an vector stores all years that has "evidence"
evi_year <- sort(unique(evi$year))
# make an vector stores how many time this word appear in each year
evi_number <- c()
for(i in 1:length(evi_year)){
 evi_number[i] <- table(evi$year)[[i]]
}

table(evi$year)
```
Then make an dataframe for those 3 words
```{r echo = FALSE}
abhijit_3word <- data.frame(word = append(rep("poor",12),
      rep("economic",24))%>% append(rep("evidence",20)),
      number = append(poor_number,econ_number)%>% append(evi_number),
      year = append(poor_year,econ_year)%>% append(evi_year))
abhijit_3word

  # png(filename = "/Users/yuelinzhou/stat133/workouts/workout3/images/3word_abhijit.png")
  #   ggplot(abhijit_3word,aes(x = year,y = number,group = word,color = word))+
  #   geom_line() +
  #   facet_wrap(~word,dir = "v") +
  #   labs(title ="3 words from abhijit")
  # dev.off()
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/3word_abhijit.png")
```
Based on the abhijit's data,I saw that the word "economics" most used in 2000-2010, the maxiumn is 6 times, which is a lot.For "evidence" is a increasing trend, it seems like in the modern day, people like to use evidence to prove something instead of keep saying such word "this will boost the economic". According to the Financial crisis from 2008, people more became poor, therefore abhijit start word "poor" more frequently.

### For angus's data I pick inequality,household,health
For the word inequality: 
```{r echo= FALSE}
# for the word inequality 
inequality <- angus[str_detect(tolower(angus$paperName),"inequality"),]
# make an vector stores all years that has "inequality"
inequality_year <- sort(unique(inequality$year))
# make an vector stores how many time this word appear in each year
inequality_number <- c()
for(i in 1:length(inequality_year)){
 inequality_number[i] <- table(inequality$year)[[i]]
}
table(inequality$year)
```

For the word household: 
```{r echo= FALSE}
house <- angus[str_detect(tolower(angus$paperName),"household"),]
# make an vector stores all years that has "household"
house_year <- sort(unique(house$year))
# make an vector stores how many time this word appear in each year
house_number <- c()
for(i in 1:length(house_year)){
 house_number[i] <- table(house$year)[[i]]
}
table(house$year)
```

For the word health:
```{r echo= FALSE}
health <- angus[str_detect(tolower(angus$paperName),"health"),]
# make an vector stores all years that has "health"
health_year <- sort(unique(health$year))
# make an vector stores how many time this word appear in each year
health_number <- c()
for(i in 1:length(health_year)){
 health_number[i] <- table(health$year)[[i]]
}
table(health$year)
```

Then I make an dataframe for those 3 words
```{r echo= FALSE}
angus_3word <- data.frame(word = append(rep("inequality",18),
      rep("household",18))%>% append(rep("health",16)),
      number = append(inequality_number,house_number)%>% append(health_number),
      year = append(inequality_year,house_year)%>% append(health_year))
angus_3word

 # png(filename = "/Users/yuelinzhou/stat133/workouts/workout3/images/3word_angus.png")
 # ggplot(angus_3word,aes(x = year,y = number,group = word,color = word))+
 #    geom_line() +
 #    facet_wrap(~word,dir = "v") +
 #   labs(title ="3 words from angus")
 # dev.off()
```

```{r echo=FALSE,fig.align= 'center'}
knitr::include_graphics("../images/3word_angus.png")
```
Based on angus's data,I saw that the word "inequality" is has 2 climaxs point in around 2003 and 2013. For the word "household" looks very similar to "inequality" that has 2 cliamxs point in 1985 and 1995. For the word "health", it start increases in 2005 all the way up to 2007,and decreases and finally remaind at the middle line. 

# Part 5: Report
```{r echo=FALSE}
library(dplyr)
```
## Q1. On average, which scholar has more co-authors?
```{r echo =FALSE}
# for co author, we will take a look at the researchers
# I create a vector to store how many co-author in each titles.
co_authors <- c()
for (i in 1:nrow(abhijit)){
co_authors[i] <- str_count(abhijit$researcher[i],",") 
}
# On average, abhijit have about 3 co-authors
mean(co_authors)

# for angus's data
co_authors2 <- c()
for (i in 1:nrow(angus)){
co_authors2[i] <- str_count(angus$researcher[i],",") 
}

# On average, angus have about 1 co-author
mean(co_authors2)
```
Based on the result, abhijit have about 2.573737 co-authors on average (I around it to 3), angus have about 1.385321 co-author on average (I around it to 1). In conclusion, abhijit has more co-authors than angus.

## Q2. Do the two scholars have mutual friends(co-authors)? If yes, print the names of their friends.
```{r echo = FALSE}
# make an vector add up all the names of the authors
mutual <- c()
for(i in 1:nrow(abhijit)){
  mutual <- append(mutual,strsplit(abhijit$researcher[i],",")[[1]])
}

# do the same thing to angus's data
mutual2 <- c()
for(i in 1:nrow(angus)){
  mutual2 <- append(mutual2,strsplit(angus$researcher[i],",")[[1]])
}

# use duplicated to find out the index of which name has appear more than 1 time
append(unique(str_trim(mutual)),
       unique(str_trim(mutual2)))[duplicated(append(unique(str_trim(mutual)),unique(str_trim(mutual2))))]
```
Based on the result, Yes,there's 59 mutual friends (excluding author himself and  "..."62-3 = 59) in the two scholars. This result suprised me because 59 mutual friends between 2 scholars make me think the world is so small, smart people are more likely to make friends with smart people. Once I graudate from berkeley, I might have a lot of friends with statistics major, so we can disccuss together to make contribution to the society.

## Q3. Did the two scholars once publish a paper together? If yes, please print the related information of that paper.
```{r echo = FALSE}
# abhijit's full name is "AV Banerjee"
# angus's full name is  “A Deaton”

# check if both names are in the researcher
# for abhijit's data
abhijit_angus <- c()
for (i in 1:nrow(abhijit)){
abhijit_angus[i] <- str_detect(abhijit$researcher[i],"AV Banerjee") && 
    str_detect(abhijit$researcher[i],"A Deaton")
}
abhijit[abhijit_angus,]

# do the same thing for angust's data
angus_abhijit <- c()
for (i in 1:nrow(angus)){
angus_abhijit[i] <- str_detect(angus$researcher[i],"AV Banerjee") && 
    str_detect(angus$researcher[i],"A Deaton")
}
angus[angus_abhijit,]
```
Based on the result, Yes, they have publish a paper together the title of their paper is called "An evaluation of World Bank research" in 2006 with 191 citations. This article is assessed by a team of 25 evaluators. Panel members also solicited views from current and past Bank staff, as well as from policy makers and academics in developing countries. Based on the evidence the authors assembled, the interviews the authors conducted, and their own consideration, the panel concluded that the World Bank needs a research department, and that its research needs cannot be fully met by hiring in from the outside. 

## Q4. What’s the paper with the most co-authors?

From Q1, I have already calucated how many co-authors in every paper
```{r echo= FALSE}
# we will be checking the maxiumn co-authors from each data set
# for abhijit's data 
co_authors
co_authors2
```

The maxiumn of co_authors in abhijit's data is 
```{r echo= FALSE}
max(co_authors)
```

The maxiumn of co_authors in angus's data is 
```{r echo= FALSE}
max(co_authors2)
```

Then find the index and display the paper title name
```{r echo= FALSE}
abhijit$paperName[which(co_authors == max(co_authors))]
angus$paperName[which(co_authors2 == max(co_authors2))]
```
Based on the result, it seems the maxiumn co-author for both scholars are 8. For abhijit's data, the paper with the most co-authors is called "Russia’s Phony Capitalism" which is about question for Russia is whether it will become a democratic oligarchy with corporatist, criminal characteristics or take the more difficult, painful road to becoming a normal, Western-style democracy with a market economy. Communism is no longer an option and so on. For angu's data, the paper with thr most co-authors is called "Attendance list" which is about low attendance are serious for all children and for the community, not just the students who miss school.

## Q5. How many distinct journals are there in the two citation tables?
```{r echo= FALSE}
# clean up journals for each scholars first 
abhijit_journals <- tolower(abhijit$journal) %>%   
  removePunctuation() %>%
  removeNumbers() %>%
  stripWhitespace()%>%
  str_trim()

angus_journals <- tolower(angus$journal) %>%   
  removePunctuation() %>%
  removeNumbers() %>%
  stripWhitespace()%>%
  str_trim()
```

### I use unique() for each scholars and display the length
For abhijit's data:
```{r echo= FALSE}
length(unique(na.omit(abhijit_journals)))
```
For angus's data:
```{r echo= FALSE}
length(unique(na.omit(angus_journals)))
```
Based on the result, Abhijit has 253 distinct journals, Angus has 265 distinct journals, clearly angus has more referenced more journals than abhijit, this shows that angus know more people, more places. But overall, 253 and 265 are seems pretty close. 

## Q6. Count the total number of citations for each journal
```{r echo= FALSE}
# add all journals first
all_journals <- tolower(append(abhijit$journal,angus$journal))

# clean the data
all_journals <- na.omit(all_journals %>%
  removePunctuation() %>%
  removeNumbers() %>%
  stripWhitespace()%>%
  str_trim())
# use unique to display all distinct journals
unique_all_journals <- unique(all_journals)

# make an for loop to store how many citations for each journals
unique_all_journals_number <- c()
for(i in 1:length(unique_all_journals)){
unique_all_journals_number[i] <- 
sum(abhijit$citations[str_detect(abhijit_journals,unique_all_journals[i])],na.rm = TRUE) + sum(angus$citations[str_detect(angus_journals,unique_all_journals[i])],na.rm = TRUE)
}

# an for loop to display the total number of citations for each journal
total_number_citations <- c()
for(i in 1:length(unique_all_journals)){
  if(unique_all_journals_number[i] == 0){
    total_number_citations[i] <- paste(unique_all_journals[i],"(total citations of:",NA,")")
  }else{
   total_number_citations[i] <- paste(unique_all_journals[i],"(total citations of:",
                                      unique_all_journals_number[i],")")
  }
}
```

Result is a very huge data set (477 elements), I only going to display the first 20 lines.
```{r echo= FALSE}
total_number_citations[1:20]
```
Based on the result, there's 477 distinct jornals for 2 scholars. each jornals with corrsponding total number of citations. from the previous question, Abhijit has 253 distinct journals, Angus has 265 distinct journals, if they combined togther will have some overlapping journals. From the data set,"the quarterly journal of economics" have the most citations of 12339, which is alot. when I search this article on google, it's about the professional and academic economists and ranking about students around the world, pretty interesting. This is everything for my workout3, thanks for grading my project, hope you enjoy it, have a good semester!

